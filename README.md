# ğŸ“Š Data Viz LLM - Version Ollama Mistral Local

**Version ultra-simplifiÃ©e - 100% GRATUITE - Aucune clÃ© API nÃ©cessaire**

Utilise votre Mistral local via Ollama.

## âš¡ Installation (2 minutes)

### 1. Installer Ollama (si pas dÃ©jÃ  fait)

```bash
# Linux/Mac
curl -fsSL https://ollama.ai/install.sh | sh

# Windows: tÃ©lÃ©charger depuis ollama.ai
```

### 2. Charger Mistral

```bash
ollama pull mistral
```

### 3. Lancer Ollama

```bash
ollama serve
```

Gardez ce terminal ouvert.

### 4. Installer l'application

```bash
# Dans un nouveau terminal
pip install -r requirements.txt
```

## ğŸš€ Utilisation

```bash
streamlit run src/app.py
```

Ouvrir http://localhost:8501

## ğŸ“ Utiliser Votre Modelfile

Vous avez crÃ©Ã© `mistral-opt.txt` avec:
```
FROM mistral
PARAMETER num_gpu 10
PARAMETER num_thread 6
PARAMETER num_ctx 2048
```

**Option 1: CrÃ©er un modÃ¨le personnalisÃ©**

```bash
# CrÃ©er le modÃ¨le
ollama create mistral-opt -f mistral-opt.txt

# Modifier src/llm/analyzer.py ligne 11:
self.model = "mistral-opt"  # Au lieu de "mistral"

# MÃªme chose dans viz_proposer.py et code_generator.py
```

**Option 2: Utiliser directement**

Les paramÃ¨tres dans votre fichier amÃ©liorent les performances:
- `num_gpu 10`: Utilise 10 GPU (si disponibles)
- `num_thread 6`: 6 threads CPU
- `num_ctx 2048`: Contexte de 2048 tokens

```bash
ollama create mistral-opt -f mistral-opt.txt
ollama run mistral-opt
```

## ğŸ¯ Workflow

1. **Upload CSV** ou choisir un exemple
2. **Saisir question**: "Quels facteurs influencent le prix ?"
3. **Analyser** (Mistral local gÃ©nÃ¨re 3 propositions)
4. **SÃ©lectionner** une visualisation
5. **TÃ©lÃ©charger** le PNG

## âœ… Avantages

- âœ… **100% Gratuit** (pas de frais API)
- âœ… **PrivÃ©** (donnÃ©es restent locales)
- âœ… **Rapide** (pas de latence rÃ©seau)
- âœ… **Pas de limite** (pas de quota)

## âš™ï¸ Optimisation

Votre `mistral-opt.txt` optimise dÃ©jÃ :
- GPU: 10 cartes (si disponibles)
- Threads: 6 CPU
- Contexte: 2048 tokens

Pour crÃ©er le modÃ¨le optimisÃ©:

```bash
ollama create mistral-opt -f mistral-opt.txt
```

Puis dans `src/llm/analyzer.py`, `viz_proposer.py`, `code_generator.py`:

```python
self.model = "mistral-opt"  
```

## ğŸ”§ Troubleshooting

**Erreur "connection refused"**
â†’ Lancer `ollama serve` dans un terminal

**RÃ©ponses lentes**
â†’ Utiliser le modÃ¨le optimisÃ© `mistral-opt`

**Erreur "model not found"**
â†’ `ollama pull mistral`

## ğŸ“ Structure Minimale

```
src/
â”œâ”€â”€ app.py              # 
â”œâ”€â”€ llm/
â”‚   â”œâ”€â”€ analyzer.py     # 
â”‚   â”œâ”€â”€ viz_proposer.py # 
â”‚   â””â”€â”€ code_generator.py # 
â”œâ”€â”€ utils/              # 
â””â”€â”€ visualization/      # 
```
